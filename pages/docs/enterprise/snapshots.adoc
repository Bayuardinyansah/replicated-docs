= Snapshots

:page-slug: /docs/admin_console/snapshots/
:page-order: 0
:page-section: Enterprise

= Snapshots overview

An important part of the lifecycle of an application is backup and restore. The admin console can be used to create and manage your storage destination and schedule, and to perform and monitor the backup and restore process. This feature is only available for licenses that have the Allow Snapshots feature enabled.

Currently, there are two types of snapshots:

* Full Snapshots (Instance)
* Partial Snapshots (Application)

Snapshots are useful for rollback and disaster recovery scenarios. They are not intended to be used for application migration scenarios.

== Full snapshots (Recommended)

Full snapshots back up the admin console and all application data.
They can be used for full Disaster Recovery; by restoring over the same instance, or into a new cluster.

There are two ways to create a full snapshot. First, make sure that your license has the snapshots feature enabled, then:

* Using the KOTS CLI [backup](/kots-cli/backup/) command.
* Using the admin console, as follows:
+
image::snapshot-instance-backup.png[Snapshot instance backup]

After a Full Snapshot has been created, you can get a command to do a restore by clicking the restore icon (check screenshot below).
There are two available options for doing a restore. You can either do a full restore which will restore the admin console and the application with all of its data, or you can choose to do a partial restore of just your application and its data.

image::snapshot-instance-restore.png[Snapshot instance restore]

If you have multiple applications within the KOTS admin console, each application should have a [backup resource](/reference/v1beta1/backup/) in order to be included in the Full Snapshot backup.

== Partial snapshots

Partial snapshots only back up applications volumes and application manifests; they do not back up the admin console or the metadata about an application.
They are great for capturing information before deploying a new release, in case you need to roll back, but they are not suitable for full disaster recovery.
For backups that give you the ability to do full Disaster Recovery, use [Full Snapshots](/kotsadm/snapshots/overview/#full-snapshots-recommended).

Partial snapshots can only be created using the admin console:

image::snapshot-application-backup.png[Snapshot application backup]

= Storage destinations

The KOTS Snapshot feature supports any compatible https://velero.io/docs/main/supported-providers/[Velero storage provider].
The admin console has built-in support for configuring AWS, GCP, Azure, S3-Compatible object store, NFS Server, or local host path as destinations.

Although embedded clusters from kURL installations are preconfigured in the admin console to store backups in the locally-provisioned object store, this is sufficient for only rollbacks and downgrades. It is not a suitable configuration for disaster recovery. We recommend that you configure a storage destination that is external to the embedded cluster in the admin console.

If the admin console is running with [minimal role-based-access-control (RBAC) privileges](/vendor/packaging/rbac/#namespace-scoped-access), the [`kots velero ensure-permissions` command](/kots-cli/velero/ensure-permissions/) must be leveraged, as the admin console requires access to the namespace in which Velero is installed.

== Prerequisites for cloud configurations

* Existing clusters: Customers must https://velero.io/docs/v1.6/basic-install/[install Velero] before configuring Snapshots.
* Embedded clusters: The vendor can provide the Velero add-on in the embedded cluster installation. If it is not provided, the Snapshots configuration dialog in the admin console notifies you to https://velero.io/docs/v1.6/basic-install/[install Velero] before you can proceed with the configuration.


== AWS

When configuring the admin console to store snapshots on AWS, the following fields are available:

[cols="1,1"]
|===
|Name |Description

|Region
|The AWS region that the S3 bucket is available in.

|Bucket
|The name of the S3 bucket to use.

|Path (Optional)
|The path in the bucket to store all snapshots in.

|Access Key ID (Optional)
|The AWS IAM Access Key ID that can read from and write to the bucket.

|Secret Access Key (Optional)
|The AWS IAM Secret Access Key that is associated with the Access Key ID.

|Use Instance Role
|When enabled, instead of providing an Access Key ID and Secret Access Key, Velero will use an instance IAM role.
|===

== GCP

When configuring the admin console to store snapshots on GCP, the following fields are available:

[cols="1,1"]
|===
|Name |Description

|Bucket
|The name of the GCP storage bucket to use.

|Path (Optional)
|The path in the bucket to store all snapshots in.

|Service Account
|The GCP IAM Service Account JSON file that has permissions to read from and write to the storage location.
|===

== Azure

When configuring the admin console to store snapshots on a Azure, the following fields are available:

[cols="1,1"]
|===
|Name |Description

|Bucket
|The name of the Azure Blob Storage Container to use.

|Path (Optional)
|The path in the Blob Storage Container to store all snapshots in.

|Resource Group
|The Resource Group name of the target Blob Storage Container.

|Storage Account
|The Storage Account Name of the target Blob Storage Container.

|Subscription ID
|The Subscription ID associated with the target Blob Storage Container (required only for access via Service Principle or AAD Pod Identity).

|Tenant ID
|The Tenant ID associated with the Azure account of the target Blob Storage container (required only for access via Service Principle).

|Client ID
|The Client ID of a Service Principle with access to the target Container (required only for access via Service Principle).

|Client Secret
|The Client Secret of a Service Principle with access to the target Container (required only for access via Service Principle).

|Cloud Name
|The Azure cloud for the target storage (options: AzurePublicCloud, AzureUSGovernmentCloud, AzureChinaCloud, AzureGermanCloud).
|===

Only connections with Service Principles are supported at this time. For more information about authentication methods and setting up Azure, see the https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure[Velero Plugin for Microsoft Azure] documentation.

== S3 compatible

When configuring the admin console to store snapshots on an S3-Compatible storage, the following fields are available:

[cols="1,1"]
|===
|Name |Description

|Region
|The AWS region that the S3 bucket is available in.

|Endpoint
|The endpoint to use to connect to the bucket.

|Bucket
|The name of the S3 bucket to use.

|Path (Optional)
|The path in the bucket to store all snapshots in.

|Access Key ID (Optional)
|The AWS IAM Access Key ID that can read from and write to the bucket.

|Secret Access Key (Optional)
|The AWS IAM Secret Access Key that is associated with the Access Key ID.

|Use Instance Role
|When enabled, instead of providing an Access Key ID and Secret Access Key, Velero will use an instance IAM role.
|===

== Network file system (NFS)

> Introduced in KOTS v1.33.0

* <<Configuring NFS>>


== Host path

> Introduced in KOTS v1.33.0

* <<Configuring a host path>>


= Configuring NFS

> Introduced in KOTS v1.33.0

The steps described in this section are only necessary if you want to configure a Network File System (NFS) as your KOTS Snapshots [storage destination](/kotsadm/snapshots/storage-destinations/).

== Prerequisites

* The NFS server must be already set up and configured to allow access from all the nodes in the cluster.
* All of the nodes in the cluster must have the necessary NFS client packages installed to be able to communicate with the NFS server. For example, the `nfs-common` package is a very common package used on Ubuntu.
* Any firewalls (if any) must be properly configured to allow traffic between the NFS server and clients (cluster nodes).

== Embedded clusters

Embedded clusters set up using installers that include the https://kurl.sh/docs/add-ons/velero[Velero] add-on are configured by default to store snapshots internally in the cluster.
There are two ways to change this configuration to use NFS:

* Using the KOTS CLI [velero configure-nfs](/kots-cli/velero/configure-nfs/) command.
* Using the admin console (Check screenshots below):

. Click the **Snapshots** tab.
. Click **Settings and Schedule**, and click the **Network File System (NFS)** dropdown option.
+
images::snapshot-destination-dropdown-nfs.png[Snapshot destination dropdown NFS]

. Enter the NFS server hostname or IP Address, and the path that is exported by the NFS server. Click **Update storage settings**. This step can take a couple of minutes to complete.
+
images::snapshot-destination-nfs-fields.png[Snapshot destination NFS fields]
+
When configuring the admin console to store snapshots on an NFS server, the following fields are available:
+
[cols="1,1"]
|===
|Name |Description

|Server
|The hostname or IP address of the NFS server.

|Path
|The path that is exported by the NFS server.
|===

== Existing clusters

NOTE: If Velero is already installed in the cluster, you can follow the same instructions mentioned in the <<Embedded clusters>> section.

If Velero is not yet installed in the cluster, the first step is to set up and deploy the necessary components that are going to be used to install and set up Velero with NFS.
This can be done in two ways:

=== Using the KOTS CLI

The [velero configure-nfs](/kots-cli/velero/configure-nfs/) CLI command can be used to configure NFS for either online or airgap installations.
After this command runs and completes successfully, it detects whether Velero is not installed and prints out specific instructions on how to install and set up Velero.

**Online Installations**

[source,terminal]
----
kubectl kots velero configure-nfs --nfs-server <hostname-or-ip> --nfs-path /path/to/directory --namespace <namespace>
----

**Airgapped Installations**

[source,terminal]
----
kubectl kots velero configure-nfs \
  --nfs-server <hostname-or-ip> \
  --nfs-path /path/to/directory \
  --namespace <namespace> \
  --kotsadm-registry private.registry.host \
  --kotsadm-namespace application-name \
  --registry-username ro-username \
  --registry-password ro-password
----

=== Using the admin console

. Click the **Snapshots** tab.
. Click the **Settings and Schedule** tab.
+
A dialog opens that contains instructions for setting up Velero with different providers.

. Click the **NFS** provider option.
+
images::snapshot-provider-nfs.png[Snapshot provider NFS]
+
A dialog opens for configuring NFS.

. Enter the NFS server hostname or IP Address, and the path that is exported by the NFS server. Click **Configure**. This step can take a few minutes to complete.
+
images::snapshot-provider-nfs-fields.png[Snapshot provider NFS fields]
+
A dialog opens and contains a CLI command that will print out instructions on how to set up Velero with the deployed NFS configuration/components.
+
images::snapshot-provider-fs-next-steps.png[Snapshot provider file system next steps]
+
. Run the CLI command and follow the instructions to install Velero.
. From the admin console, either refresh the page or click **Check for Velero** to retry detecting Velero.


= Configuring a host path

> Introduced in KOTS v1.33.0

The steps described in this section are only necessary if you want to configure a Host Path destination as your KOTS Snapshots <<Storage destination,storage destination>>.

IMPORTANT: Make sure that the host path exists and is writable by the user:group 1001:1001 on all the nodes in the cluster.

== Embedded clusters

Embedded clusters set up using installers that include the https://kurl.sh/docs/add-ons/velero[Velero] add-on are configured by default to store snapshots internally in the cluster.
There are two ways to change this configuration to use a Host Path:

* Using the KOTS CLI [velero configure-hostpath](/kots-cli/velero/configure-hostpath/) command.
* Using the admin console (Check screenshots below):

. Click the **Snapshots** tab.
. Click the **Settings and Schedule** tab, and click the **Host Path** dropdown option.
+
images::snapshot-destination-dropdown-hostpath.png[Snapshot destination dropdown host path]

. Enter the path to the directory on the node, and click **Update storage settings**. This step can take a couple of minutes to complete.
+
images::snapshot-destination-hostpath-field.png[Snapshot destination host path fields]
+
When configuring the admin console to store snapshots on a local host path, the following fields are available:

[cols="1,1"]
|===
|Name |Description

|Host Path
|A local host path on the node.
|===


== Existing clusters

If Velero is not yet installed in the cluster, the first step is to set up and deploy the necessary components that are going to be used to install and set up Velero with the provided host path.
This can be done in two ways:

* Using the KOTS CLI
* Using the admin console

NOTE: If Velero is already installed in the cluster, you can follow the same instructions mentioned in the <<Embedded clusters>> section.

=== Using the KOTS CLI

The [velero configure-hostpath](/kots-cli/velero/configure-hostpath/) CLI command can be used to configure a host path for either online or airgap installations.
After this command runs and completes successfully, it detects whether Velero is not installed and prints out specific instructions on how to install and set up Velero.

**Online Installations**

[source,terminal]
----
kubectl kots velero configure-hostpath --hostpath /path/to/directory --namespace <namespace>
----

**Airgapped Installations**

[source,terminal]
----
kubectl kots velero configure-hostpath \
  --hostpath /path/to/directory \
  --namespace <namespace> \
  --kotsadm-registry private.registry.host \
  --kotsadm-namespace application-name \
  --registry-username ro-username \
  --registry-password ro-password
----

=== Using the admin console

. Click the **Snapshots** tab.
. Click the **Settings and Schedule** tab.
+
A dialog opens that contains instructions for setting up Velero with different providers.

. Click the **Host Path** provider option (check screenshot below).
+
images::snapshot-provider-hostpath.png[Snapshot provider host path]
+
A dialog opens for configuring the host path.

. Enter the path to the directory on the node, and click **Configure**. This step can take a few minutes to complete.
+
images::snapshot-provider-hostpath-field.png[Snapshot provider host path fields]
+
 A dialog opens containing a CLI command that will print out instructions on how to set up Velero with the deployed host path configuration/components.
+
images::snapshot-provider-fs-next-steps.png[Snapshot provider file system next steps]

. Run the CLI command and follow the instructions.
. From the admin console, either refresh the page or click **Check for Velero** to retry detecting Velero.


= Schedules

The admin console contains a user interface for configuring regular snapshots.


== Retention

The default retention period for snapshots is 1 month. Setting the retention only affects snapshots created after the time of the change. For example, if an existing snapshot had a retention of 1 year and is already 6 months old, and a user then uses the UI to change the retention to 1 month, the existing snapshot will still be around for another 6 months.


== Automatic snapshots

The following screenshot contains instructions on how to enable and configure automatic scheduled snapshots.

images::snapshot-schedule.png[Snapshot Schedule]



= Partial snapshots restore

When restoring a partial snapshot, the admin console will first "undeploy" the correct application. During this process, all existing application manifests are removed from the cluster, and all PersistentVolumeClaims are deleted. This action is not reversible.

The restore process then re-deploys all application manifests to the namespace, and all pods will have an extra `initContainer` and an extra directory named `.velero`. This is used for restore hooks. For more information about the actual restore process, see the https://velero.io/docs/v1.5/restore-reference/[Velero documentation].


= Disaster recovery

To setup disaster recovery snapshots, backups should be configured to use a store that exists outside of the cluster. This is especially true for [embedded kURL installs](/kotsadm/installing/installing-embedded-cluster/).

== Restoring an existing cluster

. Begin with installing a version of Velero compatible with the one that was used to make the snapshot.
* If restoring from an network file system (NFS) or a host path storage destination, see <<Network file system (NFS)>> or a <<Configuring host path>> for the configuration steps and how to set up Velero. Otherwise, see the Velero documention for https://velero.io/docs/v1.5/basic-install/[installing] and https://velero.io/plugins/[configuring] the plugins.
+
NOTE: Restic is required and `--restic` flag must be used with `velero install` command.

. Use the KOTS CLI to [list backups](/kots-cli/backup/ls/) and [create restores](/kots-cli/restore/).

== Restoring an online embedded cluster

. Setup the [embedded cluster](/kotsadm/installing/installing-embedded-cluster/#online-installations),
. Use the KOTS CLI to configure the pre-installed velero setup to point at the snapshot storage destination. Consult the relevant CLI documentation for your provider:
+
* [AWS S3 Configuration](/kots-cli/velero/configure-aws-s3/)
* [Azure Configuration](/kots-cli/velero/configure-azure/)
* [GCP Configuration](/kots-cli/velero/configure-gcp/)
* [S3-Other Coniguration (e.g. Minio)](/kots-cli/velero/configure-other-s3/)
* <<Configuring NFS>>
* <<Configuring a hostpath>>

. Use the KOTS CLI to [list backups](/kots-cli/backup/ls/) and [create restores](/kots-cli/restore/).

== Restoring an airgapped embedded cluster

An airgapped embedded kURL cluster can be restored only if the store backend used for backups is accessible from the new cluster. kURL installer must also be able to assign the same IP address to the embedded private image registry in the new cluster. kURL installer must be provided with the correct registry IP address:

[source, terminal]
----
cat install.sh | sudo bash -s airgap kurl-registry-ip=<ip>
----

Note that the registry from the old cluster does not need to be (and should not be) accessible.

. Setup the cluster in accordance with the above guidance and [airgap cluster install documentation](/kotsadm/installing/installing-embedded-cluster/#airgapped-installations)
. Use the KOTS CLI to configure the pre-installed velero setup to point at the snapshot storage destination. Consult the relevant CLI documentation for your provider:
+
    * [AWS S3 Configuration](/kots-cli/velero/configure-aws-s3/)
    * [Azure Configuration](/kots-cli/velero/configure-azure/)
    * [GCP Configuration](/kots-cli/velero/configure-gcp/)
    * [S3-Other Coniguration (e.g. Minio)](/kots-cli/velero/configure-other-s3/)
    * <<Configuring NFS>>
    * <<Configuring hostpath>>

. Use the KOTS CLI to [list backups](/kots-cli/backup/ls/) and [create restores](/kots-cli/restore/).


= Troubleshooting

Sometimes things are working right and an installation is not able to start a backup or complete a restore. This document offers some solutions to common problems.
When a snapshot fails, a support bundle will be collected and stored automatically. Because this is a point-in-time collection of all logs and system state at the time of the failed snapshot, this is a good place to view the logs.

== Velero is crashing

If Velero is crashing and not starting, some common causes are:

==== Invalid cloud credentials

If the cloud access credentials are invalid or do not have access to the location in the configuration, Velero will crashloop. The Velero logs will be included in a support bundle, and the message will look like this.
If this is the case, recommend that the access key / secret or service account JSON are validated.

[source,terminal]
----
time="2020-04-10T14:22:24Z" level=info msg="Checking existence of namespace" logSource="pkg/cmd/server/server.go:337" namespace=velero
time="2020-04-10T14:22:24Z" level=info msg="Namespace exists" logSource="pkg/cmd/server/server.go:343" namespace=velero
time="2020-04-10T14:22:27Z" level=info msg="Checking existence of Velero custom resource definitions" logSource="pkg/cmd/server/server.go:372"
time="2020-04-10T14:22:31Z" level=info msg="All Velero custom resource definitions exist" logSource="pkg/cmd/server/server.go:406"
time="2020-04-10T14:22:31Z" level=info msg="Checking that all backup storage locations are valid" logSource="pkg/cmd/server/server.go:413"
An error occurred: some backup storage locations are invalid: backup store for location "default" is invalid: rpc error: code = Unknown desc = NoSuchBucket: The specified bucket does not exist
        status code: 404, request id: BEFAE2B9B05A2DCF, host id: YdlejsorQrn667ziO6Xr6gzwKJJ3jpZzZBMwwMIMpWj18Phfii6Za+dQ4AgfzRcxavQXYcgxRJI=
----


==== Invalid top-level directories

Another commonly seen problem in Velero starting is a reconfigured or re-used bucket. When configuring Velero to use a bucket, the bucket cannot contain other data, or else Velero will crash.
In this case, the error in the Velero logs will be:

[source,terminal]
----
time="2020-04-10T14:12:42Z" level=info msg="Checking existence of namespace" logSource="pkg/cmd/server/server.go:337" namespace=velero
time="2020-04-10T14:12:42Z" level=info msg="Namespace exists" logSource="pkg/cmd/server/server.go:343" namespace=velero
time="2020-04-10T14:12:44Z" level=info msg="Checking existence of Velero custom resource definitions" logSource="pkg/cmd/server/server.go:372"
time="2020-04-10T14:12:44Z" level=info msg="All Velero custom resource definitions exist" logSource="pkg/cmd/server/server.go:406"
time="2020-04-10T14:12:44Z" level=info msg="Checking that all backup storage locations are valid" logSource="pkg/cmd/server/server.go:413"
An error occurred: some backup storage locations are invalid: backup store for location "default" is invalid: Backup store contains invalid top-level directories: [other-directory]
----

== Snapshot restore is failing

==== Service NodePort is already allocated

Example error message:

images::snapshot-troubleshoot-service-nodeport.png[Snapshot troubleshoot service NodePort]

There is a known issue in older Kubernetes versions (< 1.19) where using a static NodePort for services can collide in multi-primary HA setup when recreating the services. you can find more details about the issue here: https://github.com/kubernetes/kubernetes/issues/85894.

This issue has been fixed in Kubernetes version 1.19, you can find more details about the fix here: https://github.com/kubernetes/kubernetes/pull/89937.

**Solution:** upgrading to Kubernetes version 1.19+ should resolve the issue.

==== Partial snapshot restore is stuck in progress

In the KOTS UI, you'll see at least one volume restore progress bar frozen at 0%. Example admin console display:

images::snapshot-troubleshoot-frozen-restore.png[Snapshot troubleshoot frozen restore]

You can confirm this is the same issue by running `kubectl get pods -n <application namespace>`, and you should see at least one pod stuck in initialization:

[source, terminal]
----
NAME                                  READY   STATUS      RESTARTS   AGE
example-mysql-0                       0/1     Init:0/2    0          4m15s  #<- the offending pod
example-nginx-77b878b4f-zwv2h         3/3     Running     0          4m15s
----

We have seen this issue with Velero version 1.5.4 and opened up https://github.com/vmware-tanzu/velero/issues/3686[this issue] with the project to inspect the root cause. However we have not experienced this using Velero 1.6.0 or greater.

**Solution:** Upgrade Velero to 1.6.0 using kURL or the https://velero.io/docs/v1.6/upgrade-to-1.6/[Velero instructions].
