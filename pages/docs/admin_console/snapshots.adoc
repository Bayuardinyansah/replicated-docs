= Snapshots

:page-slug: /docs/admin_console/snapshots/
:page-order: 0
:page-section: Admin console

= Snapshots overview

An important part of the lifecycle of an application is backup and restore. The admin console can be used to create and manage your storage destination and schedule, and to perform and monitor the backup and restore process. This feature is only available for licenses that have the Allow Snapshots feature enabled.

Currently, there are two types of snapshots:

* Full Snapshots (Instance)
* Partial Snapshots (Application)

Snapshots are useful for rollback and disaster recovery scenarios. They are not intended to be used for application migration scenarios.

== Full Snapshots (Recommended)

Full snapshots back up the admin console and all application data.
They can be used for full Disaster Recovery; by restoring over the same instance, or into a new cluster.

There are two ways to create a full snapshot. First, make sure that your license has the snapshots feature enabled, then:

* Using the KOTS CLI [backup](/kots-cli/backup/) command.
* Using the admin console (check screenshot below).

image::snapshot-instance-backup.png[Snapshot instance backup]

After a Full Snapshot has been created, you can get a command to do a restore by clicking the restore icon (check screenshot below).
There are two available options for doing a restore. You can either do a full restore which will restore the admin console and the application with all of its data, or you can choose to do a partial restore of just your application and its data.

![Instance Restore UI](/images/snapshot-instance-restore.png)

If you have multiple applications within the KOTS admin console, each application should have a [backup resource](/reference/v1beta1/backup/) in order to be included in the Full Snapshot backup.

== Partial Snapshots

Partial snapshots only back up applications volumes and application manifests; they do not back up the admin console or the metadata about an application.
They are great for capturing information before deploying a new release, in case you need to roll back, but they are not suitable for full disaster recovery.
For backups that give you the ability to do full Disaster Recovery, use [Full Snapshots](/kotsadm/snapshots/overview/#full-snapshots-recommended).

Partial snapshots can only be created via the admin console (check screenshot below).

![Application Backup UI](/images/snapshot-application-backup.png)

= Storage destinations

The KOTS Snapshot feature supports any compatible https://velero.io/docs/main/supported-providers/[Velero storage provider].
The admin console has built-in support for configuring AWS, GCP, Azure, S3-Compatible object store, NFS Server, or local host path as destinations.

Although embedded clusters from kURL installations are preconfigured in the admin console to store backups in the locally-provisioned object store, this is sufficient for only rollbacks and downgrades. It is not a suitable configuration for disaster recovery. We recommend that you configure a storage destination that is external to the embedded cluster in the admin console.

If the admin console is running with [minimal role-based-access-control (RBAC) privileges](/vendor/packaging/rbac/#namespace-scoped-access), the [`kots velero ensure-permissions` command](/kots-cli/velero/ensure-permissions/) must be leveraged, as the admin console requires access to the namespace in which Velero is installed.

== Prerequisites for cloud configurations

* Existing clusters: Customers must https://velero.io/docs/v1.6/basic-install/[install Velero] before configuring Snapshots.
* Embedded clusters: The vendor can provide the Velero add-on in the embedded cluster installation. If it is not provided, the Snapshots configuration dialog in the admin console notifies you to [install Velero](https://velero.io/docs/v1.6/basic-install/) before you can proceed with the configuration.

== AWS

When configuring the admin console to store snapshots on AWS, the following fields are available:

[cols="1,1"]
|===
| Name | Description

| Region
| The AWS region that the S3 bucket is available in

| Bucket
| The name of the S3 bucket to use

| Path (optional)
| The path in the bucket to store all snapshots in

| Access Key ID (optional)
| The AWS IAM Access Key ID that can read from and write to the bucket

| Secret Access Key (optional)
| The AWS IAM Secret Access Key that is associated with the Access Key ID

| Use Instance Role
| When enabled, instead of providing an Access Key ID and Secret Access Key, Velero will use an instance IAM role
|===

== GCP

When configuring the admin console to store snapshots on GCP, the following fields are available:

[cols="1,1"]
|===
| Name | Description

| Bucket
| The name of the GCP storage bucket to use

| Path (optional)
| The path in the bucket to store all snapshots in

| Service Account
| The GCP IAM Service Account JSON file that has permissions to read from and write to the storage location
|===


== Azure

When configuring the admin console to store snapshots on a Azure, the following fields are available:

[cols="1,1"]
|===
| Name                       | Description

| Bucket
| The name of the Azure Blob Storage Container to use

| Path (optional)
| The path in the Blob Storage Container to store all snapshots in

| Resource Group
| The Resource Group name of the target Blob Storage Container

| Storage Account
| The Storage Account Name of the target Blob Storage Container

| Subscription ID
| The Subscription ID associated with the target Blob Storage Container (required only for access via Service Principle or AAD Pod Identity) |

| Tenant ID
| The Tenant ID associated with the Azure account of the target Blob Storage container (required only for access via Service Principle)      |

| Client ID
| The Client ID of a Service Principle with access to the target Container (required only for access via Service Principle)                  |

| Client Secret
| The Client Secret of a Service Principle with access to the target Container (required only for access via Service Principle)              |

| Cloud Name
| The Azure cloud for the target storage (options: AzurePublicCloud, AzureUSGovernmentCloud, AzureChinaCloud, AzureGermanCloud)              |
|===

Only connections with Service Principles are supported at this time.
For more information about authentication methods and setting up Azure, see the https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure[Velero Plugin for Microsoft Azure] documentation.

== S3 Compatible

When configuring the admin console to store snapshots on an S3-Compatible storage, the following fields are available:

[cols="1,1"]
|===
| Name | Description

| Region
| The AWS region that the S3 bucket is available in

| Endpoint
| The endpoint to use to connect to the bucket

| Bucket
| The name of the S3 bucket to use

| Path (optional)
| The path in the bucket to store all snapshots in

| Access Key ID (optional)
| The AWS IAM Access Key ID that can read from and write to the bucket

| Secret Access Key (optional)
| The AWS IAM Secret Access Key that is associated with the Access Key ID

| Use Instance Role
| When enabled, instead of providing an Access Key ID and Secret Access Key, Velero will use an instance IAM role
|===

== Network File System (NFS)

> Introduced in KOTS v1.33.0

* [Configuring NFS](/kotsadm/snapshots/configuring-nfs/)

== Host Path

> Introduced in KOTS v1.33.0

* [Configuring a host path](/kotsadm/snapshots/configuring-hostpath/)

= Configuring NFS

> Introduced in KOTS v1.33.0

The steps described on this page are only necessary if you wish to configure a Network File System (NFS) as your KOTS Snapshots [storage destination](/kotsadm/snapshots/storage-destinations/).

Important notes before you begin:

* Make sure that you have the NFS server already set up and configured to allow access from all the nodes in the cluster.
* Make sure all the nodes in the cluster have the necessary NFS client packages installed to be able to communicate with the NFS server. For example, the `nfs-common` package is a very common package used on Ubuntu.
* Make sure that any firewalls (if any) are properly configured to allow traffic between the NFS server and clients (cluster nodes).

== Embedded Clusters

Embedded clusters set up using installers that include the https://kurl.sh/docs/add-ons/velero[Velero] add-on are configured by default to store snapshots internally in the cluster.
There are two ways to change this configuration to use NFS:

* Using the KOTS CLI [velero configure-nfs](/kots-cli/velero/configure-nfs/) command.
* Using the admin console (Check screenshots below):

First, head to the "Snapshots" tab.
From there, head to the "Settings and Schedule" tab and choose the "Network File System (NFS)" dropdown option.

![Snapshot Destination Dropdown NFS](/images/snapshot-destination-dropdown-nfs.png)

Enter the NFS server hostname or IP Address, and the path that is exported by the NFS server and click "Update storage settings".
This step might take a couple of minutes so please be patient.

![Snapshot Destination NFS Fields](/images/snapshot-destination-nfs-fields.png)

When configuring the admin console to store snapshots on an NFS server, the following fields are available:

[cols="1,1"]
|===
| Name | Description

| Server
| The hostname or IP address of the NFS server

| Path
| The path that is exported by the NFS server  |
|===


== Existing Clusters

NOTE: If Velero is already installed in the cluster, you can follow the same instructions mentioned in the [Embedded Clusters](/kotsadm/snapshots/configuring-nfs/#embedded-clusters) section.

If Velero is not yet installed in the cluster, then the first step is to set up and deploy the necessary components that are going to be used to install and set up Velero with NFS.
This can be done in two ways:

=== Using the KOTS CLI

The [velero configure-nfs](/kots-cli/velero/configure-nfs/) CLI command can be used to configure NFS for either online or airgapped installations.
After this command has run and completed successfully, it will detect if Velero is not installed and print out specific instructions on how to install and set up Velero.

**Online Installations**

[source,terminal]
----
kubectl kots velero configure-nfs --nfs-server <hostname-or-ip> --nfs-path /path/to/directory --namespace <namespace>
----

**Airgapped Installations**

[source,terminal]
----
kubectl kots velero configure-nfs \
  --nfs-server <hostname-or-ip> \
  --nfs-path /path/to/directory \
  --namespace <namespace> \
  --kotsadm-registry private.registry.host \
  --kotsadm-namespace application-name \
  --registry-username ro-username \
  --registry-password ro-password
----

=== Using the admin console

. Go to the **Snapshots** tab.
. Click the **Settings and Schedule** tab.
+
Then, you'll be presented with a dialog which contains instructions for setting up Velero with different providers.

. Click on the **NFS** provider option.
+
![Snapshot Provider NFS](/images/snapshot-provider-nfs.png)
+
Then, you'll be presented with another dialog for configuring NFS.

. Enter the NFS server hostname or IP Address, and the path that is exported by the NFS server. Click **Configure**.
+
![Snapshot Provider NFS Fields](/images/snapshot-provider-nfs-fields.png)
+
This step can take a few minutes to complete. When the configuration is successful, a different dialog opens and contains a CLI command that will print out instructions on how to set up Velero with the deployed NFS configuration/components:
+
![Snapshot Provider File System Next Steps](/images/snapshot-provider-fs-next-steps.png)
+
. After following the instructions from the above CLI command, and Velero has been installed successfully, you can go back to the admin console and either click on the "Check for Velero" button to retry detecting Velero, or simply refresh the page.
